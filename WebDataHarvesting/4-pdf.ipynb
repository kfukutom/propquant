{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "Copyright and other regulations surrounding programmatic gathering of data vary by website and jurisdiction. It is your responsibility to check all applicable laws and terms of use of any website or service before attempting any web harvesting or similar activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Extraction\n",
    "\n",
    "To extract information from text-based PDFs, we can use package PyPDF2 (https://pythonhosted.org/PyPDF2/) for text extraction\n",
    "\n",
    "## Example - US Non-farm Unemployment Rate (Weekly)\n",
    "\n",
    "<img src=\"assets/PDF-screenshot.png\" style=\"width: 720px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 1: Locate Weekly Files\n",
    "\n",
    "https://www.dol.gov    => `Topics` => `statistics` => `Employment and Unemployment` => `Unemployment Insurance Data and Statistics` => `Weekly Claims Report` => `Choose New Release and click Submit`\n",
    "\n",
    "Pattern found! \n",
    "\n",
    "Weekly files are available at `https://oui.doleta.gov/press/<YYYY>/<MM><DD><YY>.pdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U PyPDF2==1.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "import requests\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_pdf = 'https://oui.doleta.gov/press/2020/020620.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Download PDF files using `requests` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_data = requests.get(example_pdf).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Load PDF file with `PyPDF2.PdfFileReader`\n",
    "\n",
    "Check more usage at https://pythonhosted.org/PyPDF2/PdfFileReader.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PyPDF2.PdfFileReader(io.BytesIO(pdf_data))\n",
    "\n",
    "num_pages = reader.getNumPages()\n",
    "\n",
    "print(\"This PDF file has {} pages in total\".format(num_pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Extract the specific page using  `PdfFileReader.getPage(page_num)` \n",
    "\n",
    "`PdfFileReader.PageObject` represents a page in the pdf file (https://pythonhosted.org/PyPDF2/PageObject.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "page4 = reader.getPage(3)\n",
    "\n",
    "contents = page4.extractText()\n",
    "contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Extract required fields\n",
    "\n",
    "Simple rules by counting the special charater `'\\n'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = contents.split(\"\\n\")\n",
    "\n",
    "week_ending = components[2].strip()\n",
    "initial_claims_sa = components[8].strip()\n",
    "\n",
    "print(\"Week Ending: \", week_ending)\n",
    "print(\"Initial Claims (SA): \", initial_claims_sa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Create a function to perform automatic extractions from a given URL\n",
    "\n",
    "Input: pdf URL\n",
    "\n",
    "Output: Extracted `week_ending` and `initial_claims` number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import datetime\n",
    "import requests\n",
    "import PyPDF2\n",
    "\n",
    "\n",
    "def extract_us_weekly_initial_claims(url):\n",
    "    pdf_data = requests.get(url).content\n",
    "    \n",
    "    reader = PyPDF2.PdfFileReader(io.BytesIO(pdf_data))\n",
    "\n",
    "    page4 = reader.getPage(3)\n",
    "\n",
    "    contents = page4.extractText()\n",
    "    \n",
    "    components = contents.split(\"\\n\")\n",
    "\n",
    "    week_ending = components[2].strip()\n",
    "    initial_claims_sa = components[8].strip()\n",
    "    \n",
    "    return {\n",
    "        'url': url,\n",
    "        'Week Ending': week_ending,\n",
    "        'Initial Claims (SA)': initial_claims_sa\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_us_weekly_initial_claims('https://oui.doleta.gov/press/2020/020620.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_us_weekly_initial_claims('https://oui.doleta.gov/press/2020/022720.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_us_weekly_initial_claims_url(date):\n",
    "    assert isinstance(date, datetime.date)\n",
    "    return 'https://oui.doleta.gov/press/{year}/{month:02d}{day:02d}{yy:02d}.pdf'.format(\n",
    "                year=date.year, \n",
    "                month=date.month, \n",
    "                day=date.day, \n",
    "                yy=(date.year // 100)\n",
    "            ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_us_weekly_initial_claims_url(datetime.date(2020, 2, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thursdays(from_date, to_date):\n",
    "    assert isinstance(from_date, datetime.date)\n",
    "    assert isinstance(to_date, datetime.date)\n",
    "    \n",
    "    first_thursday = from_date + datetime.timedelta(days=(11-from_date.isoweekday()) % 7)\n",
    "    num_thursdays = int((to_date - first_thursday).days / 7) + 1\n",
    "    \n",
    "    return [first_thursday + datetime.timedelta(days=7*i) for i in range(num_thursdays)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thursdays(datetime.date(2020,2,1), datetime.date(2020,5,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "begin_date = datetime.date(2020,2,1)\n",
    "end_date = datetime.date(2020,5,20)\n",
    "\n",
    "pdf_urls = [get_us_weekly_initial_claims_url(date) for date in thursdays(begin_date, end_date)]\n",
    "\n",
    "pdf_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [extract_us_weekly_initial_claims(url) for url in pdf_urls]\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A More Robust Extraction Algorithm With Regular Expression\n",
    "\n",
    "Learn Regular Expression https://en.wikipedia.org/wiki/Regular_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_us_weekly_initial_claims(url):\n",
    "    pdf_data = requests.get(url).content\n",
    "    \n",
    "    reader = PyPDF2.PdfFileReader(io.BytesIO(pdf_data))\n",
    "\n",
    "    page4 = reader.getPage(3)\n",
    "\n",
    "    contents = page4.extractText()\n",
    "    \n",
    "    contents = contents.replace(\"\\n\", \" \").upper()\n",
    "    \n",
    "    # more robust value extraction using regular expression\n",
    "    week_ending = re.findall((r\"WEEK ENDING\\s+(\\w+\\s+\\d+)\"), contents)[0]\n",
    "\n",
    "    initial_claims_sa = re.findall((r\"INITIAL CLAIMS \\(SA\\)\\s+([0-9,]+)\"), contents)[0]\n",
    "    \n",
    "    return {\n",
    "        'url': url,\n",
    "        'Week Ending': week_ending,\n",
    "        'Initial Claims (SA)': initial_claims_sa\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [extract_us_weekly_initial_claims(url) for url in pdf_urls]\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
